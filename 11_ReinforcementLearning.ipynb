{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "### Understand the theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Practical achievements in the field\n",
    "* Supervised / Unsupervised / Reinforcement\n",
    "* Pavlov to Bellman\n",
    "* Environment / State / Action / Reward\n",
    "* Drawbacks - curse of dimensionality, credit assignment problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SAR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement it in practice using OpenAI's Gym\n",
    "* A handy library for learning about RL - https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work on the Cartpole problem\n",
    "#### First we make an environment in which the agent can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we implement the agent-environment loop\n",
    "* Start the process by resetting the environment\n",
    "* And return an initial observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00234686, -0.02152307,  0.03581043, -0.04828385])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_obs = env.reset()\n",
    "position = initial_obs[0]\n",
    "velocity = initial_obs[1]\n",
    "angle = initial_obs[2]\n",
    "rotation = initial_obs[3]\n",
    "initial_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The arrays\n",
    "- Starting position of pole at position 1\n",
    "- Velocity is how fast is the pole moving in order to try and balance!\n",
    "- Angle is whether the pole is leaning left or right\n",
    "- Rotation is the magnitude by which it is leaning either way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same thing by taking an action - in this case a  `step` in a given direction, 0 for left and 1 for right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00277732,  0.17306758,  0.03484476, -0.32945666])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already use the `done` boolean to work out if we can stop the loop - boolean telling us whether we've died or not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use `sample` the `action_space` space to randomly pick an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_step = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `render` the environment to see what our cart is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, but we need to build an RL agent. What next?**\n",
    "\n",
    "First, lets try to build the simplest RL agent:\n",
    "* If the pole is left, move left\n",
    "* If the pole is right, move right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_rl(env):\n",
    "    \n",
    "    #setup the game\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        # work out if my pole is on the left or on the right\n",
    "        if obs[2] < 0:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        # take an according step\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        #visualise my results\n",
    "        env.render()\n",
    "        print(obs, reward)\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # find out if I died\n",
    "        if done:\n",
    "            print(f'iterations survived {i}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01152066 -0.20595819 -0.02972786  0.32124378] 1.0\n",
      "[-0.01563982 -0.40064445 -0.02330299  0.60440544] 1.0\n",
      "[-0.02365271 -0.59543289 -0.01121488  0.88965831] 1.0\n",
      "[-0.03556137 -0.79040089  0.00657829  1.17879481] 1.0\n",
      "[-0.05136939 -0.59536497  0.03015418  0.88818127] 1.0\n",
      "[-0.06327669 -0.40066496  0.04791781  0.60512801] 1.0\n",
      "[-0.07128999 -0.2062447   0.06002037  0.3279148 ] 1.0\n",
      "[-0.07541488 -0.0120263   0.06657866  0.05474719] 1.0\n",
      "[-0.07565541  0.18208096  0.06767361 -0.21620896] 1.0\n",
      "[-0.07201379  0.37617348  0.06334943 -0.48680046] 1.0\n",
      "[-0.06449032  0.57034703  0.05361342 -0.75886427] 1.0\n",
      "[-0.05308338  0.76469083  0.03843613 -1.03420631] 1.0\n",
      "[-0.03778956  0.95928117  0.01775201 -1.3145788 ] 1.0\n",
      "[-0.01860394  1.15417402 -0.00853957 -1.60165319] 1.0\n",
      "[ 0.00447954  0.9591542  -0.04057263 -1.31164473] 1.0\n",
      "[ 0.02366263  0.76456882 -0.06680553 -1.03193195] 1.0\n",
      "[ 0.038954    0.57039607 -0.08744417 -0.76094875] 1.0\n",
      "[ 0.05036193  0.37658065 -0.10266314 -0.49701224] 1.0\n",
      "[ 0.05789354  0.18304474 -0.11260338 -0.23836687] 1.0\n",
      "[ 0.06155443 -0.01030353 -0.11737072  0.01678333] 1.0\n",
      "[ 0.06134836 -0.20356375 -0.11703506  0.27025138] 1.0\n",
      "[ 0.05727709 -0.39683803 -0.11163003  0.52384977] 1.0\n",
      "[ 0.04934033 -0.59022653 -0.10115303  0.77937589] 1.0\n",
      "[ 0.0375358  -0.78382307 -0.08556551  1.03859801] 1.0\n",
      "[ 0.02185934 -0.97771019 -0.06479355  1.30323911] 1.0\n",
      "[ 0.00230513 -1.17195321 -0.03872877  1.57495627] 1.0\n",
      "[-0.02113393 -1.36659264 -0.00722965  1.85531307] 1.0\n",
      "[-0.04846579 -1.56163444  0.02987661  2.14574263] 1.0\n",
      "[-0.07969847 -1.36681925  0.07279147  1.86243267] 1.0\n",
      "[-0.10703486 -1.17256661  0.11004012  1.59320713] 1.0\n",
      "[-0.13048619 -0.97890903  0.14190426  1.33676424] 1.0\n",
      "[-0.15006437 -0.785831    0.16863955  1.09163827] 1.0\n",
      "[-0.16578099 -0.59328375  0.19047231  0.85625997] 1.0\n",
      "[-0.17764667 -0.40119622  0.20759751  0.62900041] 1.0\n",
      "[-0.18567059 -0.20948308  0.22017752  0.40820135] 1.0\n",
      "iterations survived 34\n"
     ]
    }
   ],
   "source": [
    "naive_rl(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can do better than that! Lets build a model which learns to move better based on training data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we need some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_training_data(env):\n",
    "    \n",
    "    number_of_games = 100\n",
    "    last_moves = 10\n",
    "    observations = []\n",
    "    actions = []\n",
    "    \n",
    "    for i in range(number_of_games):\n",
    "        game_obs = []\n",
    "        game_acts = []\n",
    "        obs = env.reset()\n",
    "    \n",
    "    for j in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        game_obs.append(obs)\n",
    "        game_acts.append(action)\n",
    "        \n",
    "        if done:\n",
    "            observations += game_obs[:-(last_moves+1)]\n",
    "            actions += game_acts[1:-last_moves]\n",
    "            break\n",
    "    \n",
    "    observations = np.array(observations)\n",
    "    actions = np.array(actions)\n",
    "    \n",
    "    return observations, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then a model which plays based on its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_rl(env, m):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        # start to play game\n",
    "        # let my ML model tell me what to do next\n",
    "        \n",
    "        obs = obs.reshape(-1, 4)\n",
    "        action = int(m.predict(obs))\n",
    "        \n",
    "        #take an according step\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        #visualise my results\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # find out if I died\n",
    "        if done:\n",
    "            print(f'iterations survived {i}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets run the code, and measure the improvement\n",
    "* Setup the gym\n",
    "* Collect training data\n",
    "* Train a model\n",
    "* And play\n",
    "* And measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximcondon/anaconda3/envs/deep_learning/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations survived 199\n"
     ]
    }
   ],
   "source": [
    "X, y = collect_training_data(env)\n",
    "m = RandomForestClassifier()\n",
    "m.fit(X, y)\n",
    "\n",
    "smart_rl(env, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Much improved on the original attempt! Can then try other models or start to optimise hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
